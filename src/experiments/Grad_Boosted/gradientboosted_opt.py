# -*- coding: utf-8 -*-
"""GradientBoosted_Opt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hwmvmuI6AfQugaDUQ4RLbGo2S3C61wWH
"""


import numpy as np


#Organize Data
import seaborn as sns; sns.set()

from sklearn.ensemble import GradientBoostingRegressor


from lifelines.utils import concordance_index
from sklearn.metrics import mean_squared_error, r2_score

import pickle5 as pickle

"""#Paths"""

#paths
#featues
ecfp_train = np.load('data/folds/hyperparaSearch/train_ecfp_hyper_std.npy')
seq_train = np.load('data/folds/hyperparaSearch/train_seq_hyper_std.npy')
ecfp_val = np.load('data/folds/hyperparaSearch/val_ecfp_hyper_std.npy')
seq_val = np.load('data/folds/hyperparaSearch/val_seq_hyper_std.npy')
#Affinities
train_affi = np.load('data/folds/hyperparaSearch/train_affi.npy')
val_affi = np.load('data/folds/hyperparaSearch/val_affi.npy')

"""#Get Labels and ids"""

#Split like in Fully Connected Deep Neural Net from hyperparasearch
# Training
mol_ids_train = train_affi[:, 0]
target_ids_train = train_affi[:, 1]
labels_train = train_affi[:, 2].astype('float32')
print(f'Len Labels Train:\t{len(labels_train)}')

# Validation
mol_ids_val = val_affi[:, 0]
target_ids_val = val_affi[:, 1]
labels_val = val_affi[:, 2].astype('float32')
print(f'Len Labels Val:\t{len(labels_val)}')

"""#Combine features"""

features_train = np.concatenate((ecfp_train, seq_train), axis=1)
print(f'Len Train:\t{len(features_train)}')
print(f'Len Feature Vec:\t{len(features_train[0])}')

features_val = np.concatenate((ecfp_val, seq_val), axis=1)
print(f'Len Train:\t{len(features_val)}')
print(f'Len Feature Vec:\t{len(features_val[0])}')

"""#Hyperpara Search"""

random_search =RandomizedSearchCV(GradientBoostingRegressor(loss='squared_error'), param_grid, random_state=1, n_iter=50, cv=5, verbose=3, n_jobs=1)
random_search.fit(train_data, y_train_KIBA)
#random_search.best_params_

"""#Train on Full train data"""

#Fit from prev hyperpara run
best_param = {'n_estimators': 600,
              'min_samples_split': 10,
              'min_samples_leaf': 8,
              'max_features': 'sqrt',
              'max_depth': 15,
              'learning_rate': 0.01}
classifier_KIBA = GradientBoostingRegressor(**random_search.best_params_)
#classifier_KIBA = GradientBoostingRegressor(**best_param)
classifier_KIBA.fit(features_train, labels_train)

pred_KIBA=classifier_KIBA.predict(features_val)

"""#Metrics"""

def r_squared_error(y_obs,y_pred):
    y_obs = np.array(y_obs)
    y_pred = np.array(y_pred)
    y_obs_mean = [np.mean(y_obs) for y in y_obs]
    y_pred_mean = [np.mean(y_pred) for y in y_pred]

    mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))
    mult = mult * mult

    y_obs_sq = sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))
    y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean) )

    return mult / float(y_obs_sq * y_pred_sq)

def squared_error_zero(y_obs,y_pred):
    k = get_k(y_obs,y_pred)

    y_obs = np.array(y_obs)
    y_pred = np.array(y_pred)
    y_obs_mean = [np.mean(y_obs) for y in y_obs]
    upp = sum((y_obs - (k*y_pred)) * (y_obs - (k* y_pred)))
    down= sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))

    return 1 - (upp / float(down))

def get_k(y_obs,y_pred):
    y_obs = np.array(y_obs)
    y_pred = np.array(y_pred)

    return sum(y_obs*y_pred) / float(sum(y_pred*y_pred))

def get_rm2(ys_orig,ys_line):
    r2 = r_squared_error(ys_orig, ys_line)
    r02 = squared_error_zero(ys_orig, ys_line)
    return r2 * (1 - np.sqrt(np.absolute((r2*r2)-(r02*r02))))
    
import torch 
def concordance_index_taskwise(predictions, labels, target_ids):
    CIs = list()
    target_id_list = list()
    predictions = torch.tensor(predictions)
    labels = torch.tensor(labels)
    target_ids = torch.tensor(target_ids)

    for target_idx in torch.unique(target_ids):
        rows = torch.where(target_ids == target_idx)
        preds = predictions[rows]
        y = labels[rows]

        if torch.unique(y).shape[0] >= 2:
            ci = concordance_index(y, preds)
            CIs.append(ci)
            target_id_list.append(target_idx.item())
        else:
            CIs.append(np.nan)
            target_id_list.append(target_idx.item())
    return np.nanmean(CIs), CIs, target_id_list

pred_KIBA_train=classifier_KIBA.predict(features_train)
print("KIBA-Train")
ci_train, CIs_train, _ = concordance_index_taskwise(pred_KIBA_train, labels_train, target_ids_train)
print(f'Concordance index:\t {ci_train}')
print(f"MSE\t\t：{mean_squared_error(labels_train, pred_KIBA_train)}")
print(f"rm2\t\t：{get_rm2(labels_train, pred_KIBA_train)}")

#Nicht Taskwise
concordance_index(labels_train, pred_KIBA_train)

print("KIBA-Val")
ci, CIs, _ = concordance_index_taskwise(pred_KIBA, labels_val, target_ids_val)
print(f'Concordance index:\t {ci}')
print(f"MSE\t\t：{mean_squared_error(labels_val, pred_KIBA)}")
print(f"rm2\t\t：{get_rm2(labels_val, pred_KIBA)}")

#Nicht Taskwise
concordance_index(labels_val, pred_KIBA)

len(CIs)
CIs

